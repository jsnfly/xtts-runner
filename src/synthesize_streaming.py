import argparse
import logging
import numpy as np
from pathlib import Path
import pyaudio
import time
import torch
from torch.functional import F
from transformers.generation.streamers import BaseStreamer

from gpt import XTTSGPT
from tokenizer import TextTokenizer

audio = pyaudio.PyAudio()
AUDIO_FORMAT = pyaudio.paFloat32  # https://en.wikipedia.org/wiki/Audio_bit_depth
NUM_CHANNELS = 1  # Number of audio channels

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

class XTTSStreamer(BaseStreamer):

    def __init__(self, all_latents, decoder, speaker_embedding, speed, chunk_size):
        self.all_latents = all_latents
        self.chunk_size = chunk_size
        self.decoder = decoder
        self.speaker_embedding = speaker_embedding.to(decoder.device)

        length_scale = 1.0 / speed
        self.decoder.ar_mel_length_compression *= length_scale

        self.generated_tokens = []
        self.generated_audio_bytes = b''
        self.audio_stream = None
        self.played_audio_len = 0

        pointer = 0
        bytes_per_sample = audio.get_sample_size(AUDIO_FORMAT)
        def callback(_, frame_count, *args):
            nonlocal pointer

            end_pointer = pointer + frame_count * NUM_CHANNELS * bytes_per_sample
            chunk = self.generated_audio_bytes[pointer:end_pointer]
            pointer = end_pointer

            # Automatically stops when too few bytes are returned.
            return chunk, pyaudio.paContinue

        self.callback = callback

    def put(self, token):
        self.generated_tokens.append(token)
        if len(self.generated_tokens) % self.chunk_size == 0:
            self.generate_audio()
            if self.audio_stream is None:
                self.start_stream()

    def end(self):
        self.generate_audio()
        while self.audio_stream.is_active():
            time.sleep(0.1)

        self.audio_stream.stop_stream()
        self.audio_stream = None

    def generate_audio(self):
        generated_latents = torch.cat(self.all_latents, dim=1)[:, -len(self.generated_tokens):]
        with torch.no_grad():
            generated_audio = self.decoder(generated_latents, g=self.speaker_embedding)

        # TODO: handle chunks?
        # (https://github.com/coqui-ai/TTS/blob/dbf1a08a0d4e47fdad6172e433eeb34bc6b13b4e/TTS/tts/models/xtts.py#L688)
        bytes_ = generated_audio.squeeze().cpu().numpy().tobytes()
        self.generated_audio_bytes += bytes_[len(self.generated_audio_bytes):]
        logging.debug(f"Generated audio. Current number of generated bytes: {len(self.generated_audio_bytes)}")


    def start_stream(self):
        self.audio_stream = audio.open(
            format=AUDIO_FORMAT,
            channels=NUM_CHANNELS,
            rate=24_000,
            output=True,
            stream_callback=self.callback
        )


def main(model_dir: str, text: str, lang: str, speaker: str, speed: float, chunk_size: int):
    text_tokenizer = TextTokenizer(str(Path(model_dir) / "vocab.json"))

    gpt = XTTSGPT(Path(model_dir) / "config.json")
    gpt.load(Path(model_dir) / "model.pth")
    gpt.to(DEVICE)
    gpt.set_speaker_embeddings(Path(model_dir) / "speakers_xtts.pth", speaker)
    logging.debug("Model weights loaded")

    token_encoding = text_tokenizer.encode(text, lang)
    logging.debug(f"Text tokens: {token_encoding.tokens}")
    input_ids = torch.tensor(token_encoding.ids + [gpt.config.gpt_start_audio_token], dtype=torch.int64).unsqueeze(0)
    logging.debug(f"Input ids: {input_ids}")

    # List to store the last hidden states during generation. Could also be returned as part of the result, but this is
    # more flexible, esp. for audio streaming.
    all_latents = []

    streamer = XTTSStreamer(all_latents, gpt.decoder, gpt.speaker_emb, speed, chunk_size)
    gpt.generate(
        input_ids.to(DEVICE),
        bos_token_id=gpt.config.gpt_start_audio_token,
        pad_token_id=gpt.config.gpt_stop_audio_token,
        eos_token_id=gpt.config.gpt_stop_audio_token,
        do_sample=True,
        top_p=0.85,
        top_k=50,
        temperature=0.75,
        num_return_sequences=1,
        num_beams=1,
        length_penalty=1.0,
        repetition_penalty=5.0,
        max_new_tokens=gpt.config.gpt_max_audio_tokens,
        return_dict_in_generate=True,
        output_hidden_states=True,
        all_latents=all_latents,
        streamer=streamer
    )
    logging.debug("Done generating")
    audio.terminate()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Synthesize speech from text using XTTSGPT model.')
    parser.add_argument('model_dir', type=str, help='Path to the directory containing the model files.')
    parser.add_argument('--text', type=str, required=True, help='The text to synthesize speech from.')
    parser.add_argument('--lang', type=str, default="en", help='The language of the input text. Default is "en".')
    parser.add_argument('--speaker', type=str, default="Aaron Dreschner", help='Speaker voice.')

    # If the playback speed is faster than the generation speed, the PyAudio-Stream will run out of data at some point.
    # In that case the stream will stop and not produce any more audio.
    parser.add_argument('--speed', type=float, default=1.0, help='Playback speed. Default is 1.0.')

    parser.add_argument('--chunk_size', type=int, default=20,
                        help='Number of generated audio tokens to trigger audio generation. Default is 20.')

    args = parser.parse_args()
    main(args.model_dir, args.text, args.lang, args.speaker, args.speed, args.chunk_size)
