{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1794da-3a1a-4be5-8f63-dce6658fa42e",
   "metadata": {},
   "source": [
    "# XttsConfig\n",
    "\n",
    "* dataclass\n",
    "* inherits from `BaseTTSConfig`\n",
    "    * which inherits from `BaseTrainingConfig`\n",
    "    * which inherits from `TrainerConfig` which is part of the  [\"coqui-tts-trainer\" package](https://github.com/idiap/coqui-ai-Trainer)\n",
    "    * which inherits from `Coqpit` which is part of the [\"coqpit\" package](https://github.com/coqui-ai/coqpit)\n",
    "    * which inherits from `Serializable` (part of \"coqpit\") and `MutableMapping` (part of the standard library)\n",
    "    * only the classes in \"coqpit\" provide methods, the others only add fields\n",
    "* `#load_json` loads the json file as dict and calls `self = self.deserialize(dump_dict)` and then `self.check_values()`\n",
    "    * `self = self.deserialize(dump_dict)` creates the dataclass from the dict.\n",
    "    * `self.check_values()` does nothing as it is only a dummy method by default and is not overwritten by the subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa4861-a1fd-4a88-aac6-30791485a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = XttsConfig()\n",
    "# config.load_json(\"/path/to/xtts/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db9587-f9b2-49ba-8409-15fe5f85b38b",
   "metadata": {},
   "source": [
    "# Xtts\n",
    "\n",
    "* Xtts model implementation (only supports inference)\n",
    "* inherits from `BaseTTS`\n",
    "    * which inhertis from `BaseTrainerModel`\n",
    "    * which inherits from `TrainerModel` which is an abstract class which requires the methods `init_from_config`, `inference` and `load_checkpoint` to be defined by subclasses (and that's all it does)\n",
    "    * which inherits from `TrainerModel` which is part of the  [\"coqui-tts-trainer\" package](https://github.com/idiap/coqui-ai-Trainer)\n",
    "    * which inherits from `ABC` and `nn.Module` and also defines mostly abstract methods\n",
    "* `#init_from_config`\n",
    "    * just calls `Xtts(config)`\n",
    "* `#__init__`\n",
    "    * calls `super().__init__(config, ap=None, tokenizer=None)`\n",
    "        * super method just sets some instance attributes (directly or via `self._set_model_args(config)`)\n",
    "    * sets the tokenizer and some instance attributes\n",
    "    * calls `self.init_models()`\n",
    "        * sets up `self.gpt` and `self.hifigan_decoder` \n",
    "    * calls `self.register_buffer(\"mel_stats\", torch.ones(80))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5feddfc-9b85-4122-b476-5669f3cc65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Xtts.init_from_config(config)\n",
    "# model.load_checkpoint(config, checkpoint_dir=\"/path/to/xtts/\", eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7982976-6b6a-42ce-bd86-fd095afa0106",
   "metadata": {},
   "source": [
    "# Xtts#synthesize\n",
    "\n",
    "* sets some settings\n",
    "* calls `self.inference(text, language, gpt_cond_latent, speaker_embedding, **settings)` (if speaker embeddings are loaded from file, let's focus on that for now)\n",
    "    * tokenizes the text\n",
    "    * calls `self.gpt.generate(...)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806d5ed-bb98-491f-9b01-d0a3e06f47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model.synthesize(\n",
    "#     \"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\n",
    "#     config,\n",
    "#     speaker_wav=\"/data/TTS-public/_refclips/3.wav\",\n",
    "#     gpt_cond_len=3,\n",
    "#     language=\"en\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2090f-dd5e-4501-b34d-8a308db9244c",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "```python\n",
    "    def encode(self, txt, lang):\n",
    "        lang = lang.split(\"-\")[0]  # remove the region\n",
    "        self.check_input_length(txt, lang)\n",
    "        txt = self.preprocess_text(txt, lang)\n",
    "        lang = \"zh-cn\" if lang == \"zh\" else lang\n",
    "        txt = f\"[{lang}]{txt}\"\n",
    "        txt = txt.replace(\" \", \"[SPACE]\")\n",
    "        return self.tokenizer.encode(txt).ids\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b78700-96dc-4ef1-af43-ad4de2f72efe",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c848a-fa30-429d-befc-ba327cc37665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcc1d8-773a-42b8-9316-cdb0ebb1d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"./XTTS-v2\")\n",
    "SPEAKER = \"Aaron Dreschner\"\n",
    "LANG = \"en\"\n",
    "TEXT = \"Hi! This is a test!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343d23d-5af7-49ae-8200-f48e3cef10f8",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb5f7b-6c73-458c-91af-23cdaedff2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa71fff-cacb-463a-b1e5-3d9389081dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(MODEL_PATH / \"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "print(json.dumps(config, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee152ead-10d4-414f-8a3e-5f567932997b",
   "metadata": {},
   "source": [
    "## Load Speaker Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc948d1-c979-4033-9350-63b7e6be8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch==2.5.1\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3b72a-6906-4e93-bcfb-1f3969259d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = torch.load(MODEL_PATH / \"speakers_xtts.pth\", weights_only=True)\n",
    "print(sorted(speakers.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354756a8-b094-45b0-b59b-eeaed5dfca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_cond_latent = speakers[SPEAKER]['gpt_cond_latent']\n",
    "speaker_embedding = speakers[SPEAKER]['speaker_embedding']\n",
    "print(gpt_cond_latent.shape, speaker_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe9aba-780f-41b0-b6bb-9a661f042a6d",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d0e11-d4ca-4ff5-b423-35a308de599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers==4.46.2 (for tokenizers==0.20.3)\n",
    "import re\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9057cb6-861a-4684-a426-e3353bd587aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceBpeTokenizer:\n",
    "    CHAR_LIMITS = {\n",
    "        \"en\": 250,\n",
    "        \"de\": 253,\n",
    "        \"fr\": 273,\n",
    "        \"es\": 239,\n",
    "        \"it\": 213,\n",
    "        \"pt\": 203,\n",
    "        \"pl\": 224,\n",
    "        \"zh\": 82,\n",
    "        \"ar\": 166,\n",
    "        \"cs\": 186,\n",
    "        \"ru\": 182,\n",
    "        \"nl\": 251,\n",
    "        \"tr\": 226,\n",
    "        \"ja\": 71,\n",
    "        \"hu\": 224,\n",
    "        \"ko\": 95,\n",
    "        \"hi\": 150,\n",
    "    }\n",
    "\n",
    "    def __init__(self, vocab_file):\n",
    "        self.tokenizer = Tokenizer.from_file(str(vocab_file))\n",
    "\n",
    "    def encode(self, text, lang):\n",
    "        lang = lang.split(\"-\")[0]  # remove the region\n",
    "        self.check_input_length(text, lang)\n",
    "        text = self.preprocess_text(text)\n",
    "        text = f\"[{lang}]{text}\"\n",
    "        text = text.replace(\" \", \"[SPACE]\")\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "    def check_input_length(self, text, lang):\n",
    "        limit = self.CHAR_LIMITS.get(lang, 250)\n",
    "        if len(text) > limit:\n",
    "            print(f\"The text length exceeds the character limit of {limit} for language '{lang}', this might cause truncated audio.\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Original has some more stuff, but seems incomplete even there.\n",
    "        return re.sub(r\"\\s+\", \" \", text.lower().replace('\"', \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4396a2-7cb4-4e37-858f-07d44c292455",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = VoiceBpeTokenizer(MODEL_PATH / \"vocab.json\")\n",
    "token_encoding = tokenizer.encode(TEXT, LANG)\n",
    "print(token_encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc6def-c0f3-4694-acb4-a6bb6a325288",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tokenizer.tokenizer.get_vocab().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19358b1-5c78-4fa5-a2a3-070bf0aa911f",
   "metadata": {},
   "source": [
    "## Initialize GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27390efb-7c30-4e64-bcbb-d815c799d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from transformers import GenerationMixin, GPT2PreTrainedModel\n",
    "from functools import partial\n",
    "\n",
    "def null_position_embeddings(range_, dim):\n",
    "    return torch.zeros((range_.shape[0], range_.shape[1], dim), device=range_.device)\n",
    "\n",
    "class LearnedPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, seq_len, model_dim, init=0.02, relative=False):\n",
    "        super().__init__()\n",
    "        # nn.Embedding\n",
    "        self.emb = torch.nn.Embedding(seq_len, model_dim)\n",
    "        # Initializing this way is standard for GPT-2\n",
    "        self.emb.weight.data.normal_(mean=0.0, std=init)\n",
    "        self.relative = relative\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        sl = x.shape[1]\n",
    "        if self.relative:\n",
    "            start = random.randint(sl, self.seq_len) - sl\n",
    "            return self.emb(torch.arange(start, start + sl, device=x.device))\n",
    "        else:\n",
    "            return self.emb(torch.arange(0, sl, device=x.device))\n",
    "\n",
    "    def get_fixed_embedding(self, ind, dev):\n",
    "        return self.emb(torch.tensor([ind], device=dev)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1614d-98c4-48c0-a07f-c538b7e578b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config[\"model_args\"]\n",
    "\n",
    "class GPT2InferenceModel(GPT2PreTrainedModel, GenerationMixin):\n",
    "    max_seq_len = model_config[\"gpt_max_audio_tokens\"] + model_config[\"gpt_max_text_tokens\"] + model_config[\"gpt_max_prompt_tokens\"] + 1\n",
    "    gpt_config = GPT2Config(\n",
    "        vocab_size=model_config[\"gpt_max_audio_tokens\"],\n",
    "        n_positions=max_seq_len,\n",
    "        n_ctx=max_seq_len,\n",
    "        n_embd=model_config[\"gpt_n_model_channels\"],\n",
    "        n_layer=model_config[\"gpt_layers\"],\n",
    "        n_head=model_config[\"gpt_n_heads\"],\n",
    "        gradient_checkpointing=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(self.gpt_config)\n",
    "\n",
    "        self.text_emb = nn.Embedding(model_config[\"gpt_number_text_tokens\"], self.gpt_config.n_embd)\n",
    "        self.text_pos_emb = LearnedPositionEmbeddings(model_config[\"gpt_max_text_tokens\"] + 2, self.gpt_config.n_embd)\n",
    "        self.mel_emb = nn.Embedding(model_config[\"gpt_num_audio_tokens\"], self.gpt_config.n_embd)\n",
    "        self.mel_pos_emb = LearnedPositionEmbeddings(model_config[\"gpt_max_audio_tokens\"] + 2 + 1, self.gpt_config.n_embd)\n",
    "        \n",
    "        self.transformer = GPT2Model(self.gpt_config)\n",
    "        del self.transformer.wpe\n",
    "        self.transformer.wpe = partial(null_position_embeddings, dim=self.gpt_config.n_embd)\n",
    "        # Built-in token embeddings are unused.\n",
    "        del self.transformer.wte\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(self.gpt_config.n_embd)\n",
    "        self.mel_head = nn.Linear(self.gpt_config.n_embd, model_config[\"gpt_num_audio_tokens\"])\n",
    "        \n",
    "\n",
    "    def compute_embeddings(self, cond_latents, text_tokens):\n",
    "        # Add start/end token to the start/end of the sequences. \n",
    "        text_tokens = F.pad(text_tokens, (1, 0), value=tokenizer.tokenizer.token_to_id('[START]'))\n",
    "        text_tokens = F.pad(text_tokens, (0, 1), value=tokenizer.tokenizer.token_to_id('[STOP]'))\n",
    "        \n",
    "        emb = self.text_emb(text_tokens) + self.text_pos_emb(text_tokens)\n",
    "        emb = torch.cat([cond_latents, emb], dim=1)\n",
    "        self.cached_prefix_emb = emb\n",
    "        gpt_inputs = torch.full(\n",
    "            (\n",
    "                emb.shape[0],\n",
    "                emb.shape[1] + 1,  # +1 for the start_audio_token\n",
    "            ),\n",
    "            fill_value=1,\n",
    "            dtype=torch.long,\n",
    "            device=text_tokens.device,\n",
    "        )\n",
    "        gpt_inputs[:, -1] = model_config[\"gpt_start_audio_token\"]\n",
    "        return gpt_inputs\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids, past_key_values=None, *args, **kwargs):\n",
    "        prefix_emb = self.cached_prefix_emb\n",
    "        prefix_len = prefix_emb.shape[1]\n",
    "        if input_ids.shape[-1] != 1:\n",
    "            # Only for first generation step.\n",
    "            gen_inputs = input_ids[:, prefix_len:]\n",
    "            gen_emb = self.mel_emb(gen_inputs)\n",
    "            gen_emb = gen_emb + self.mel_pos_emb(gen_emb)\n",
    "            emb = torch.cat([prefix_emb, gen_emb], dim=1)\n",
    "        else:\n",
    "            emb = self.mel_emb(input_ids)\n",
    "            emb = emb + self.mel_pos_emb.get_fixed_embedding(\n",
    "                attention_mask.shape[1] - (prefix_len + 1), attention_mask.device\n",
    "            )\n",
    "        outputs = self.transformer(inputs_embeds=emb, attention_mask=attention_mask,\n",
    "                                   position_ids=position_ids, past_key_values=past_key_values,\n",
    "                                   use_cache=True)\n",
    "        outputs.logits = self.mel_head(self.final_norm(outputs.last_hidden_state))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb947101-5cff-457e-984f-d1b4db6f05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT2InferenceModel()\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f7005-7dc1-43ad-8f5a-c8a2b414a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = torch.tensor(token_encoding.ids, dtype=torch.int32).unsqueeze(0)\n",
    "cond_latents = gpt_cond_latent\n",
    "input_ids = gpt.compute_embeddings(cond_latents, text_tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3088e-96ad-4c52-b141-ee49243fd2aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = gpt.forward(input_ids)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148572c-006b-4f44-98d5-9115e9c252c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gpt.generate(\n",
    "    input_ids,\n",
    "    bos_token_id=model_config[\"gpt_start_audio_token\"],\n",
    "    pad_token_id=model_config[\"gpt_stop_audio_token\"],\n",
    "    eos_token_id=model_config[\"gpt_stop_audio_token\"],\n",
    "    max_length=100 # TODO\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da5ad0-5666-4524-818a-64c218b58b7a",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46830ba-1207-4916-9dda-d1bb6e886896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import io\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "class CustomDict(dict):\n",
    "    @property\n",
    "    def __dict__(self):\n",
    "        return self\n",
    "\n",
    "class TensorWrapper:\n",
    "    def __init__(self, size, dtype=torch.float32, device='cpu'):\n",
    "        self.size = size\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        \n",
    "    def set_(self, storage, offset, size, stride):\n",
    "        if not isinstance(size, tuple):\n",
    "            size = tuple(size)\n",
    "        # Handle empty size case\n",
    "        if len(size) == 0:\n",
    "            return torch.tensor(storage.data[offset], dtype=storage.dtype)\n",
    "        # Handle scalar case\n",
    "        if size == (1,):\n",
    "            return torch.tensor(storage.data[offset], dtype=storage.dtype)\n",
    "        \n",
    "        # Calculate total number of elements needed based on size\n",
    "        total_elements = np.prod(size)\n",
    "        \n",
    "        # Slice the storage data using the offset\n",
    "        data_slice = storage.data[offset:offset + total_elements]\n",
    "        \n",
    "        # Regular case with proper offset handling\n",
    "        return torch.from_numpy(data_slice).to(storage.dtype).view(size)\n",
    "\n",
    "class CustomStorage:\n",
    "    def __init__(self, data, dtype):\n",
    "        self.dtype = dtype\n",
    "        self.data = data\n",
    "        \n",
    "    @property\n",
    "    def _untyped_storage(self):\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return 'cpu'\n",
    "\n",
    "class CustomUnpickler(pickle.Unpickler):\n",
    "    \"\"\"Custom unpickler that substitutes dummy classes for missing ones.\"\"\"\n",
    "    \n",
    "    DTYPE_MAP = {\n",
    "        'FloatStorage': (torch.float32, np.float32),\n",
    "        'LongStorage': (torch.int64, np.int64),\n",
    "        'IntStorage': (torch.int32, np.int32),\n",
    "        'BoolStorage': (torch.bool, np.bool_),\n",
    "    }\n",
    "    \n",
    "    def __init__(self, file, zip_archive):\n",
    "        super().__init__(file)\n",
    "        self.zip_archive = zip_archive\n",
    "        \n",
    "    def persistent_load(self, pid):\n",
    "        \"\"\"Handle persistent ID loading by returning a dummy storage with actual data.\"\"\"\n",
    "        print(f\"Persistent load: {pid}\")\n",
    "        if isinstance(pid, tuple) and pid[0] == 'storage':\n",
    "            storage_type, storage_class, key, location, numel = pid\n",
    "            \n",
    "            # Handle the case where storage_class is already CustomStorage\n",
    "            if isinstance(storage_class, type) and storage_class.__name__ == 'CustomStorage':\n",
    "                storage_name = 'FloatStorage'  # default to float if we get CustomStorage\n",
    "            else:\n",
    "                storage_name = storage_class.__name__\n",
    "            \n",
    "            torch_dtype, np_dtype = self.DTYPE_MAP.get(storage_name, (torch.float32, np.float32))\n",
    "            \n",
    "            try:\n",
    "                tensor_data = self.load_tensor_data(key, numel, np_dtype)\n",
    "                return CustomStorage(tensor_data, dtype=torch_dtype)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load tensor data: {e}\")\n",
    "                return CustomStorage(\n",
    "                    np.zeros(numel, dtype=np_dtype), \n",
    "                    dtype=torch_dtype\n",
    "                )\n",
    "                \n",
    "        return pid[1]\n",
    "        \n",
    "    def load_tensor_data(self, key, numel, dtype):\n",
    "        \"\"\"Load tensor data from the zip file.\"\"\"\n",
    "        data_file = f'model/data/{key}'\n",
    "        with self.zip_archive.open(data_file, 'r') as f:\n",
    "            data_bytes = f.read()\n",
    "            return np.frombuffer(data_bytes, dtype=dtype)\n",
    "        \n",
    "    def find_class(self, module, name):\n",
    "        \"\"\"Override find_class to return dummy classes for missing ones.\"\"\"\n",
    "        print(f\"Finding: {module}.{name}\")\n",
    "        try:\n",
    "            if module == 'torch._utils' and name == '_rebuild_tensor_v2':\n",
    "                return self._rebuild_tensor_v2\n",
    "            if module == 'torch._utils' and name == '_rebuild_tensor':\n",
    "                return self._rebuild_tensor\n",
    "            if module == 'torch' and name in self.DTYPE_MAP:\n",
    "                return CustomStorage\n",
    "            return super().find_class(module, name)\n",
    "        except:\n",
    "            return CustomDict\n",
    "            \n",
    "    def _rebuild_tensor_v2(self, storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata=None):\n",
    "        \"\"\"Custom tensor rebuilding function.\"\"\"\n",
    "        tensor = self._rebuild_tensor(storage, storage_offset, size, stride)\n",
    "        tensor.requires_grad = requires_grad\n",
    "        return tensor\n",
    "    \n",
    "    def _rebuild_tensor(self, storage, storage_offset, size, stride):\n",
    "        \"\"\"Create a new tensor with the given size and data.\"\"\"\n",
    "        wrapper = TensorWrapper(size, dtype=storage.dtype)\n",
    "        return wrapper.set_(storage, storage_offset, size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb783ad0-3d64-47d8-8277-9feecdfebb12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(MODEL_PATH / \"model.pth\", 'r') as archive:\n",
    "    with archive.open('model/data.pkl', 'r') as f:\n",
    "        data = CustomUnpickler(f, archive).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a907fe7-5e05-48d3-b434-07fb90bfc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a0bde-5dd2-40ba-b896-4cfd9fd421e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "gpt_state_dict = {re.sub(r\"^gpt\\.gpt\", \"transformer\", key): value for key, value in data['model'].items() if key.startswith(\"gpt.gpt.\")}\n",
    "gpt_state_dict['text_emb.weight'] = data['model']['gpt.text_embedding.weight']\n",
    "gpt_state_dict['text_pos_emb.emb.weight'] = data['model']['gpt.text_pos_embedding.emb.weight']\n",
    "gpt_state_dict['mel_emb.weight'] = data['model']['gpt.mel_embedding.weight']\n",
    "gpt_state_dict['mel_pos_emb.emb.weight'] = data['model']['gpt.mel_pos_embedding.emb.weight']\n",
    "gpt_state_dict['final_norm.weight'] = data['model']['gpt.final_norm.weight']\n",
    "gpt_state_dict['final_norm.bias'] = data['model']['gpt.final_norm.bias']\n",
    "gpt_state_dict['mel_head.weight'] = data['model']['gpt.mel_head.weight']\n",
    "gpt_state_dict['mel_head.bias'] = data['model']['gpt.mel_head.bias']\n",
    "\n",
    "assert set(p[0] for p in gpt.named_parameters()) == set(gpt_state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f01495-f7e9-4194-bf66-94ffd5ffd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.load_state_dict(gpt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd45c6-2974-45f2-b950-8a5bfb7114b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = torch.tensor(token_encoding.ids, dtype=torch.int32).unsqueeze(0)\n",
    "cond_latents = gpt_cond_latent\n",
    "input_ids = gpt.compute_embeddings(cond_latents, text_tokens)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = gpt.generate(\n",
    "        input_ids,\n",
    "        bos_token_id=model_config[\"gpt_start_audio_token\"],\n",
    "        pad_token_id=model_config[\"gpt_stop_audio_token\"],\n",
    "        eos_token_id=model_config[\"gpt_stop_audio_token\"],\n",
    "        do_sample=False,\n",
    "        top_p=0.85,\n",
    "        top_k=50,\n",
    "        temperature=0.75,\n",
    "        num_return_sequences=1,\n",
    "        num_beams=1,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=5.0,\n",
    "        max_new_tokens=model_config['gpt_max_audio_tokens']\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd018a-70da-4ea3-9c3f-060d14cb627e",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aba4b2-04bc-4b69-a05a-57365e0ce16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = F.pad(text_tokens, (1, 0), value=tokenizer.tokenizer.token_to_id('[START]'))\n",
    "text_inputs = F.pad(text_inputs, (0, 1), value=tokenizer.tokenizer.token_to_id('[STOP]'))\n",
    "text_emb = gpt.text_emb(text_inputs) + gpt.text_pos_emb(text_inputs)\n",
    "\n",
    "gpt_codes = output[:, input_ids.shape[1]:]\n",
    "code_stride_len = model_config['gpt_code_stride_len']\n",
    "expected_output_len = gpt_codes.shape[-1] * code_stride_len\n",
    "code_lengths = torch.ceil(torch.tensor([expected_output_len]) / code_stride_len).long() + 3\n",
    "max_mel_len = code_lengths.max()\n",
    "audio_codes = F.pad(gpt_codes, (0, max_mel_len - gpt_codes.shape[-1]))\n",
    "audio_codes = F.pad(audio_codes[:, :max_mel_len], (0, 1), value=model_config[\"gpt_stop_audio_token\"])\n",
    "audio_codes[0, code_lengths[0] - 3:] = model_config[\"gpt_stop_audio_token\"]\n",
    "audio_codes = F.pad(audio_codes, (1, 0), value=model_config[\"gpt_start_audio_token\"])\n",
    "audio_codes = F.pad(audio_codes, (0, 1), value=model_config[\"gpt_stop_audio_token\"])\n",
    "\n",
    "mel_emb = gpt.mel_emb(audio_codes) + gpt.mel_pos_emb(audio_codes)\n",
    "\n",
    "emb = torch.cat([cond_latents, text_emb, mel_emb], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b54a8f-4ac6-47d2-9880-a2297a9d4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_out = gpt.transformer(inputs_embeds=emb, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a274f95-e637-44cf-9481-5c08e5f63d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = gpt.final_norm(gpt_out.last_hidden_state[:, cond_latents.shape[1]:])\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407541a2-0f83-4e56-9855-52e2e217b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_logits = enc[:, -mel_emb.shape[1]:]  # These are not really logits, but latents\n",
    "mel_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d211272-a1be-4def-abd2-88fd95e2bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"coqui-ai-TTS/TTS/tts/layers/xtts\")\n",
    "\n",
    "from hifigan_decoder import HifiDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56354f9-5253-4fa7-8c8b-e153edf7e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_latents = mel_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44de63-84c8-4c8c-afb4-fd210f4a8160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hifigan_decoder = HifiDecoder()\n",
    "state_dict = {key[len('hifigan_decoder.'):]: value for key, value in data['model'].items() if key.startswith(\"hifigan_decoder.\")}\n",
    "hifigan_decoder.load_state_dict(state_dict)\n",
    "hifigan_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d39ae4-7580-4d85-b2da-456701984a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = hifigan_decoder(gpt_latents, g=speaker_embedding).cpu().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd6c35-e67a-46e9-b7fd-370c6488f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "audio_widget = Audio(data=wav.detach().numpy(), rate=24000)\n",
    "display(audio_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6eec5-5339-4305-8c87-8f0272f279d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
